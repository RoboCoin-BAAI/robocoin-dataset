#!/usr/bin/env python3
"""
ROS Bag æ•°æ®é›†ç»“æ„æ¢æµ‹è„šæœ¬

ç”¨äºåˆ†æå®é™…çš„rosbagæ•°æ®é›†ç»“æ„ï¼ŒåŒ…æ‹¬ï¼š
1. bagæ–‡ä»¶ä¸­çš„topicåˆ—è¡¨
2. å„topicçš„æ¶ˆæ¯ç±»å‹å’Œæ•°é‡
3. JSONå…ƒæ•°æ®æ–‡ä»¶çš„å†…å®¹
4. ç”Ÿæˆé€‚åˆçš„é…ç½®æ–‡ä»¶æ¨¡æ¿
"""

import json
from pathlib import Path
from typing import Dict, List, Tuple
import yaml

try:
    from rosbags.highlevel import AnyReader
    HAS_ROSBAGS = True
except ImportError:
    print("âš ï¸  rosbagsåº“æœªå®‰è£…ï¼Œæ— æ³•è§£æbagæ–‡ä»¶å†…å®¹")
    HAS_ROSBAGS = False


def analyze_dataset_structure(dataset_path: Path) -> Dict:
    """
    åˆ†ææ•°æ®é›†ç›®å½•ç»“æ„
    
    Args:
        dataset_path: æ•°æ®é›†æ ¹ç›®å½•
        
    Returns:
        dict: åŒ…å«åˆ†æç»“æœçš„å­—å…¸
    """
    if not dataset_path.exists():
        raise FileNotFoundError(f"æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}")
    
    analysis = {
        'dataset_path': str(dataset_path),
        'bag_files': [],
        'json_files': [],
        'topics_analysis': {},
        'metadata_analysis': {},
        'recommended_config': {}
    }
    
    # æŸ¥æ‰¾bagæ–‡ä»¶
    bag_files = []
    for pattern in ['*.bag', '*.mcap', '*.db3']:
        bag_files.extend(list(dataset_path.rglob(pattern)))
    
    analysis['bag_files'] = [str(f) for f in bag_files]
    
    # æŸ¥æ‰¾JSONæ–‡ä»¶
    json_files = list(dataset_path.rglob('*.json'))
    analysis['json_files'] = [str(f) for f in json_files]
    
    # åˆ†æbagæ–‡ä»¶å†…å®¹
    if HAS_ROSBAGS and bag_files:
        print(f"ğŸ” åˆ†æ {len(bag_files)} ä¸ªbagæ–‡ä»¶...")
        analysis['topics_analysis'] = analyze_bag_topics(bag_files[:3])  # åªåˆ†æå‰3ä¸ªæ–‡ä»¶
    
    # åˆ†æJSONå…ƒæ•°æ®
    if json_files:
        print(f"ğŸ“‹ åˆ†æ {len(json_files)} ä¸ªJSONæ–‡ä»¶...")
        analysis['metadata_analysis'] = analyze_json_metadata(json_files[:5])  # åªåˆ†æå‰5ä¸ªæ–‡ä»¶
    
    # ç”Ÿæˆæ¨èé…ç½®
    analysis['recommended_config'] = generate_recommended_config(analysis)
    
    return analysis


def analyze_bag_topics(bag_files: List[Path]) -> Dict:
    """åˆ†æbagæ–‡ä»¶ä¸­çš„topics"""
    topics_info = {}
    
    for bag_file in bag_files:
        print(f"  ğŸ“ åˆ†ææ–‡ä»¶: {bag_file.name}")
        
        try:
            with AnyReader([bag_file]) as reader:
                # è·å–è¿æ¥ä¿¡æ¯
                for connection in reader.connections.values():
                    topic = connection.topic
                    if topic not in topics_info:
                        topics_info[topic] = {
                            'msgtype': connection.msgtype,
                            'message_counts': [],
                            'files': []
                        }
                    
                    # ç»Ÿè®¡æ¶ˆæ¯æ•°é‡
                    msg_count = 0
                    for conn, timestamp, rawdata in reader.messages():
                        if conn.topic == topic:
                            msg_count += 1
                    
                    topics_info[topic]['message_counts'].append(msg_count)
                    topics_info[topic]['files'].append(str(bag_file))
                    
        except Exception as e:
            print(f"    âŒ è§£æå¤±è´¥: {e}")
            continue
    
    # æ±‡æ€»ç»Ÿè®¡
    for topic, info in topics_info.items():
        info['total_messages'] = sum(info['message_counts'])
        info['avg_messages_per_file'] = info['total_messages'] / len(info['message_counts']) if info['message_counts'] else 0
        info['files_count'] = len(info['files'])
    
    return topics_info


def analyze_json_metadata(json_files: List[Path]) -> Dict:
    """åˆ†æJSONå…ƒæ•°æ®æ–‡ä»¶"""
    metadata_samples = []
    common_keys = set()
    
    for json_file in json_files:
        print(f"  ğŸ“„ åˆ†ææ–‡ä»¶: {json_file.name}")
        
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                metadata_samples.append({
                    'file': str(json_file),
                    'keys': list(data.keys()),
                    'sample_data': {k: str(v)[:100] + ('...' if len(str(v)) > 100 else '') 
                                   for k, v in data.items()}
                })
                
                if not common_keys:
                    common_keys = set(data.keys())
                else:
                    common_keys &= set(data.keys())
                    
        except Exception as e:
            print(f"    âŒ è§£æå¤±è´¥: {e}")
            continue
    
    return {
        'samples': metadata_samples[:3],  # åªä¿ç•™å‰3ä¸ªæ ·æœ¬
        'common_keys': list(common_keys),
        'total_files': len(json_files)
    }


def generate_recommended_config(analysis: Dict) -> Dict:
    """åŸºäºåˆ†æç»“æœç”Ÿæˆæ¨èé…ç½®"""
    config = {
        'fps': 30,  # é»˜è®¤å€¼ï¼Œéœ€è¦æ ¹æ®å®é™…è°ƒæ•´
        'features': {
            'observation': {
                'images': [],
                'state': {
                    'sub_state': []
                }
            },
            'action': {
                'sub_action': []
            }
        }
    }
    
    # æ ¹æ®topicsåˆ†æç”Ÿæˆå›¾åƒé…ç½®
    topics = analysis.get('topics_analysis', {})
    cam_counter = 1
    
    for topic, info in topics.items():
        msgtype = info.get('msgtype', '')
        
        # å›¾åƒtopics
        if 'Image' in msgtype or 'image' in topic.lower():
            cam_name = f"cam_{cam_counter}"
            if 'left' in topic.lower():
                cam_name += "_left"
            elif 'right' in topic.lower():
                cam_name += "_right"
            elif 'head' in topic.lower():
                cam_name += "_head"
            
            config['features']['observation']['images'].append({
                'cam_name': cam_name,
                'args': {
                    'topic_name': topic
                }
            })
            cam_counter += 1
        
        # çŠ¶æ€topicsï¼ˆå…³èŠ‚çŠ¶æ€ç­‰ï¼‰
        elif 'JointState' in msgtype or 'joint' in topic.lower():
            config['features']['observation']['state']['sub_state'].append({
                'names': [f'joint_{i}' for i in range(1, 8)],  # å‡è®¾7ä¸ªå…³èŠ‚
                'args': {
                    'topic_name': topic,
                    'range_from': 0,
                    'range_to': 7
                }
            })
    
    return config


def generate_report(analysis: Dict, output_path: Path = None):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    report = []
    report.append("# ROS Bag æ•°æ®é›†ç»“æ„åˆ†ææŠ¥å‘Š")
    report.append("=" * 50)
    report.append("")
    
    report.append(f"ğŸ“ æ•°æ®é›†è·¯å¾„: {analysis['dataset_path']}")
    report.append(f"ğŸ“¦ Bagæ–‡ä»¶æ•°é‡: {len(analysis['bag_files'])}")
    report.append(f"ğŸ“„ JSONæ–‡ä»¶æ•°é‡: {len(analysis['json_files'])}")
    report.append("")
    
    # Topicsåˆ†æ
    if analysis['topics_analysis']:
        report.append("ğŸ¯ Topics åˆ†æ:")
        for topic, info in analysis['topics_analysis'].items():
            report.append(f"  â€¢ {topic}")
            report.append(f"    ç±»å‹: {info['msgtype']}")
            report.append(f"    æ€»æ¶ˆæ¯æ•°: {info['total_messages']}")
            report.append(f"    å¹³å‡æ¶ˆæ¯æ•°/æ–‡ä»¶: {info['avg_messages_per_file']:.1f}")
            report.append("")
    
    # å…ƒæ•°æ®åˆ†æ
    if analysis['metadata_analysis']:
        report.append("ğŸ“‹ JSON å…ƒæ•°æ®åˆ†æ:")
        common_keys = analysis['metadata_analysis'].get('common_keys', [])
        if common_keys:
            report.append(f"  é€šç”¨å­—æ®µ: {', '.join(common_keys)}")
            report.append("")
            
            # æ˜¾ç¤ºæ ·æœ¬æ•°æ®
            for sample in analysis['metadata_analysis']['samples']:
                report.append(f"  ğŸ“„ {Path(sample['file']).name}:")
                for key, value in sample['sample_data'].items():
                    report.append(f"    {key}: {value}")
                report.append("")
    
    # æ¨èé…ç½®
    report.append("ğŸ› ï¸  æ¨èé…ç½® (convertor_config.yaml):")
    report.append("```yaml")
    config_yaml = yaml.dump(analysis['recommended_config'], default_flow_style=False, indent=2)
    report.append(config_yaml)
    report.append("```")
    
    report_text = "\n".join(report)
    
    if output_path:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        print(f"ğŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
    else:
        print(report_text)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ROS Bag æ•°æ®é›†ç»“æ„æ¢æµ‹å·¥å…·")
    print("=" * 50)
    
    # è¿™é‡Œéœ€è¦æŒ‡å®šå®é™…çš„æ•°æ®é›†è·¯å¾„
    dataset_path = Path("/home/diy01/dev/dataset_example")
    
    if not dataset_path.exists():
        print("âŒ è¯·ä¿®æ”¹ dataset_path ä¸ºå®é™…çš„æ•°æ®é›†è·¯å¾„")
        print("ğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
        print("   1. ä¿®æ”¹è„šæœ¬ä¸­çš„ dataset_path å˜é‡")
        print("   2. ç¡®ä¿å®‰è£…äº† rosbags åº“: pip install rosbags")
        print("   3. é‡æ–°è¿è¡Œè„šæœ¬")
        return
    
    try:
        # æ‰§è¡Œåˆ†æ
        analysis = analyze_dataset_structure(dataset_path)
        
        # ç”ŸæˆæŠ¥å‘Š
        report_path = Path("rosbag_dataset_analysis_report.md")
        generate_report(analysis, report_path)
        
        print("\nğŸ‰ åˆ†æå®Œæˆï¼")
        print("ğŸ’¡ æ¥ä¸‹æ¥çš„æ­¥éª¤:")
        print("   1. æŸ¥çœ‹ç”Ÿæˆçš„åˆ†ææŠ¥å‘Š")
        print("   2. æ ¹æ®æ¨èé…ç½®è°ƒæ•´ convertor_config_*.yaml")
        print("   3. æµ‹è¯•è½¬æ¢æµç¨‹")
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
